{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SHWETA WAGHMARE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMCVaSkI7qb9eQe0FYgqFA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wshweta23/Shweta-Waghmare-1/blob/main/SHWETA_WAGHMARE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxecIXUPibtr"
      },
      "source": [
        "Table of Contents\n",
        "1  Libraries\n",
        "2  Functions\n",
        "3  Data Prep\n",
        "4  Terrorism Around the World\n",
        "5  Countries and Terrorism\n",
        "6  NLP on Incident Summary\n",
        "6.1  Regular Expressions\n",
        "6.1.1  Sites and Hiperlinks\n",
        "6.1.2  Numbers\n",
        "6.1.3  Special Characteres\n",
        "6.1.4  Additional Whitespaces\n",
        "6.2  Lower Case\n",
        "6.3  WordCloud\n",
        "7  References\n",
        "This kernel is a continuation of the kernel Global Terrorism - Data Understanding, where an extremely important process of understanding the data was carried out in order to go deep into the context and make useful data transformations for further analysis.\n",
        "This second approach intends to tell a story about Terrorism around the world taking the data provided as a basis. Here, we will apply an exploratory data analysis, look for patterns and explanations related to the context and present the conclusions in a dynamic and visual ways. If you like this kernel, please upvote! This always keep me motivated to do things even better! English is not my mother language... so sorry for any mistake!\n",
        "\n",
        "We will use libs like Folium, Seaborn, Matplotlib and other usefull tools to try to see:\n",
        "\n",
        "A big picture of terrorism around the world and its evolution over the years;\n",
        "Countries with most incidents recorded;\n",
        "Countries with highest number of victims;\n",
        "A dashboard for terrorism analysis in some countries;\n",
        "Incidents that lasted more than 24h (extended = 1);\n",
        "Major radical groups responsible for terrorist attacks (gname);\n",
        "Attacks with the highest number of terrorists (nperps);\n",
        "A WordCloud for attributes like summary corp1, target1 and motive;\n",
        "Libraries\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import folium\n",
        "from folium.plugins import FastMarkerCluster, Fullscreen, MiniMap, HeatMap, HeatMapWithTime\n",
        "import geopandas as gpd\n",
        "from branca.colormap import LinearColormap\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import re\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "Functions\n",
        "Reading the Data\n",
        "In previous versions, I separated the analysis into \"Data Preparation\" and \"Storytelling\" just to keep in mind that some transformations in data would be necessary to reach the goals. But, as long as the storytelling part took the center stage and grew on large scale, I decided to make the transformations on data only where applicable.\n",
        "\n",
        "In other words, all the steps required for the analysis will be performed on their respective topic. For now, let's just read the data, filter attributes and go through visualizations!\n",
        "\n",
        "terr = pd.read_csv('../input/globalterrorismdb_0718dist.csv', encoding='ISO-8859-1')\n",
        "attribs = ['eventid', 'iyear', 'imonth', 'iday', 'extended', 'country_txt', 'region_txt', 'city', \n",
        "                        'latitude', 'longitude', 'specificity', 'summary', 'success', 'suicide', 'attacktype1_txt', \n",
        "                        'targtype1_txt', 'copr1', 'target1', 'natlty1_txt', 'gname', 'motive', 'nperps', \n",
        "                        'weaptype1_txt', 'nkill', 'nkillter', 'nwound', 'nwoundte', 'ishostkid', 'nhostkid']\n",
        "terr_data = terr.loc[:, attribs]\n",
        "terr_data.head()\n",
        "eventid\tiyear\timonth\tiday\textended\tcountry_txt\tregion_txt\tcity\tlatitude\tlongitude\t...\tgname\tmotive\tnperps\tweaptype1_txt\tnkill\tnkillter\tnwound\tnwoundte\tishostkid\tnhostkid\n",
        "0\t197000000001\t1970\t7\t2\t0\tDominican Republic\tCentral America & Caribbean\tSanto Domingo\t18.456792\t-69.951164\t...\tMANO-D\tNaN\tNaN\tUnknown\t1.0\tNaN\t0.0\tNaN\t0.0\tNaN\n",
        "1\t197000000002\t1970\t0\t0\t0\tMexico\tNorth America\tMexico city\t19.371887\t-99.086624\t...\t23rd of September Communist League\tNaN\t7.0\tUnknown\t0.0\tNaN\t0.0\tNaN\t1.0\t1.0\n",
        "2\t197001000001\t1970\t1\t0\t0\tPhilippines\tSoutheast Asia\tUnknown\t15.478598\t120.599741\t...\tUnknown\tNaN\tNaN\tUnknown\t1.0\tNaN\t0.0\tNaN\t0.0\tNaN\n",
        "3\t197001000002\t1970\t1\t0\t0\tGreece\tWestern Europe\tAthens\t37.997490\t23.762728\t...\tUnknown\tNaN\tNaN\tExplosives\tNaN\tNaN\tNaN\tNaN\t0.0\tNaN\n",
        "4\t197001000003\t1970\t1\t0\t0\tJapan\tEast Asia\tFukouka\t33.580412\t130.396361\t...\tUnknown\tNaN\tNaN\tIncendiary\tNaN\tNaN\tNaN\tNaN\t0.0\tNaN\n",
        "5 rows Ã— 29 columns\n",
        "\n",
        "Before we get started, we need to ensure the charts will be plotted the way we expect. First, we have to set the name of United States country to same string that we have in the json file used as map (United States of America). Second, let's just change the large string describing the \"Vehicle\" in weaptype1_txt attribute.\n",
        "\n",
        "terr_data['country_txt'] = terr_data['country_txt'].apply(lambda x: x.replace('United States', \n",
        "                                                                              'United States of America'))\n",
        "terr_data['weaptype1_txt'] = terr_data['weaptype1_txt'].apply(lambda x: x.split()[0] if 'Vehicle' in x.split() else x)\n",
        "If you want to see details of a full process of global terrorism data preparation, please go through the Global Terrorism - Data Understanding.\n",
        "\n",
        "Terrorism Around the World\n",
        "\n",
        "Well, here we can see clearly that Iraq is the country with the highest number of incidents recorded. The map also shows tooltips with the name of the country, number of incidents and total of victims recorded. Another thing that can be said looking at the map is that the Middle East and South Asia are the regions with the highes number of recorded attacks between 1970 and 2017.\n",
        "\n",
        "\n",
        "With this Heatmap, we can see places with greater concentration of terrorism incidents from 1970 to 2017. To get more insights, let's see the\"evolution\" of terrorism year by year.\n",
        "\n",
        "\n",
        "Now we have a selection bar in the bottom of the map where we can select the terrorist records from a specific year between 1970 and 2017. It is important to cross this information with historical facts, wars and incidents. It's kind of interesting to see how the concentration of incidents starts at North America in the 70s and move to Europe and Middle East region as long as the time goes by.\n",
        "\n",
        "The most recent data we have is from 2017. Let's plot a global heatmap to see incidents among the months of 2017. Using the selection bar in the bottom, we can see the concentration of terrorism from january to december of 2017.\n",
        "\n",
        "\n",
        "Countries and Terrorism\n",
        "Let's change the scenery and see the effects of terrorism in specific countries. First of all, let's take a look at the main countries affected by terrorism.\n",
        "\n",
        "\n",
        "As we have already seen in our first geographical plot, the highest concentration of incidentes recorded are from Middle East & North Africa. The region represents 27.8% of all records between 1970 and 2017.\n",
        "\n",
        "In the next plot, we will make a comparison of this historical data with 2017 data, but this time looking at the top 10 countries if highest nuber of terrorist incidents.\n",
        "\n",
        "\n",
        "With the grap above we can see that Iraq and Afghanistan are the countries with most terrorism occurences in 2017 (and also in all period). Colombia, Peru and El Salvador appear in historica data but don't appear in 2017 data maybe because of past conflicts. Let's make a more specific analysis in some countries to see more details.\n",
        "\n",
        "Now I present you the dashboard for country-terrorism relationship analysis. We will see details from Iraq, United States, Nigeria, Colombia and Egypt.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "NLP on Incident Summary\n",
        "Well, in this session our goal is to apply some Natural Language Processing techniques to take a look at the words as a tool for understanding the Terrorism around the World. First of all, let's see some elements from the column summary\n",
        "\n",
        "terr_data['summary'][:10]\n",
        "0                                                  NaN\n",
        "1                                                  NaN\n",
        "2                                                  NaN\n",
        "3                                                  NaN\n",
        "4                                                  NaN\n",
        "5    1/1/1970: Unknown African American assailants ...\n",
        "6                                                  NaN\n",
        "7    1/2/1970: Unknown perpetrators detonated explo...\n",
        "8    1/2/1970: Karl Armstrong, a member of the New ...\n",
        "9    1/3/1970: Karl Armstrong, a member of the New ...\n",
        "Name: summary, dtype: object\n",
        "We can see two points by now:\n",
        "\n",
        "It is necessary to handle null data on this\n",
        "Also, we have to eliminate the date information at the beginning of each instance\n",
        "temp_corpus = terr_data['summary'].dropna()\n",
        "corpus = temp_corpus.apply(lambda x: x.split(': ')[-1]).values\n",
        "print(f'We have {len(corpus)} elements on the corpus\\n\\n')\n",
        "print(f'Example 1: \\n{corpus[1]}\\n')\n",
        "print(f'Example 2: \\n{corpus[-1]}')\n",
        "We have 115562 elements on the corpus\n",
        "\n",
        "\n",
        "Example 1: \n",
        "Unknown perpetrators detonated explosives at the Pacific Gas & Electric Company Edes substation in Oakland, California, United States.  Three transformers were damaged costing an estimated $20,000 to $25,000.  There were no casualties.\n",
        "\n",
        "Example 2: \n",
        "An explosive device was discovered and defused at a plaza in Cotabato City, Maguindanao, Philippines. No group claimed responsibility for the incident.\n",
        "Regular Expressions\n",
        "Now that we have already transformed the corpus on an array structure, let's apply some Regular Expressions to deal with non-desired elements. The analysis we will do in the following topics will cover:\n",
        "\n",
        "Search for Sites and Hiperlinks\n",
        "Search for Numbers\n",
        "Search for Special Characteres\n",
        "Search for Additional Whitespaces\n",
        "Sites and Hiperlinks\n",
        "For this task, we will iterate all over the corpus applying the method findall using a specific Regular Expression created for searching sites and hiperlinks. Let's see what we can tell about it.\n",
        "\n",
        "for c in corpus:\n",
        "    urls = re.findall('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', c)\n",
        "    if len(urls) == 0:\n",
        "        pass\n",
        "    else:\n",
        "        print(f'Description: {list(corpus).index(c)} - Links: {urls}')\n",
        "Description: 6977 - Links: [('http', 'www.earthliberationfront.com', '/news/2002/020116ml.html')]\n",
        "Well, it seems that only one attack summary have hiperlinks on it. Let's print it just to confirm.\n",
        "\n",
        "# Example\n",
        "corpus[6977]\n",
        "' ALF cites this incident as an anti-globalization action in their 2001 year-end Direct Action Report, available as of 13 January 2003 at http://www.earthliberationfront.com/news/2002/020116ml.html.'\n",
        "Ok, our method worked. So we will replace the links with the token \"link\".\n",
        "\n",
        "# Replacing sites and hiperlinks\n",
        "corpus_wo_hiperlinks = []\n",
        "for c in corpus:\n",
        "    c = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', 'link', c)\n",
        "    corpus_wo_hiperlinks.append(c)\n",
        "corpus_wo_hiperlinks[6977]\n",
        "' ALF cites this incident as an anti-globalization action in their 2001 year-end Direct Action Report, available as of 13 January 2003 at link.'\n",
        "Good! Keep going.\n",
        "\n",
        "Numbers\n",
        "As we could see on some examples above, there are incidents descriptions with numbers. Here we will search for those text descriptions and replace the numbers with the token number.\n",
        "\n",
        "# Example of description with number\n",
        "corpus_wo_hiperlinks[399]\n",
        "'At around 8:00 AM, a member of the Black Liberation Army flagged down a police officer driving his radio car in Manhattan, New York, United States.  When the cop pulled over, the perpetrator began stabbing the officer with a knife.  The officer then shot and killed the assailant.  That morning, the New York Times received two anonymous calls stating that a New York policeman would be killed at 8:00 AM unless the BLA received $100,000.'\n",
        "# Replacing numbers\n",
        "corpus_wo_numbers = []\n",
        "for c in corpus_wo_hiperlinks:\n",
        "    c = re.sub('\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'number', c)\n",
        "    corpus_wo_numbers.append(c)\n",
        "corpus_wo_numbers[399]\n",
        "'At around number:number AM, a member of the Black Liberation Army flagged down a police officer driving his radio car in Manhattan, New York, United States.  When the cop pulled over, the perpetrator began stabbing the officer with a knife.  The officer then shot and killed the assailant.  That morning, the New York Times received two anonymous calls stating that a New York policeman would be killed at number:number AM unless the BLA received $number,number.'\n",
        "We are one step closer to the goal.\n",
        "\n",
        "Special Characteres\n",
        "Again, as long as we are looking at incidents descriptions, we faced some special characteres that need to be replaced for further analysis. For this, let's apply another Regular Expresion to replace special characters with whitespace.\n",
        "\n",
        "# Example with special characteres\n",
        "corpus_wo_numbers[1113]\n",
        "'Members of the Evan Mecham Eco-Terrorist International Conspiracy (EMETIC), claimed responsibility for damaging the pylons and cables which supported the main ski chairlift at the Fairfield Snow Bowl Ski Resort in Flagstaff, Arizona in the United States,   There were no casualties, but the property damage was estimated at $number,number and an additional $number,number to heighten security at the resort. In a letter to the ski resort, the group stated that \"if trees were not allowed to grow back\" and \"ski operations not discontinued,\" then more damage would occur. Marc Leslie Davis, Margaret Katherine Millet, Ilse Washington Asplund, and Marc Andre Baker were arrested and in number, pled guilty and sentenced for their involvement in this incident.'\n",
        "# Replacing special characteres with whitespace\n",
        "corpus_text = []\n",
        "for c in corpus_wo_numbers:\n",
        "    c = re.sub(r'\\W', ' ', c)\n",
        "    corpus_text.append(c)\n",
        "corpus_text[1113]\n",
        "'Members of the Evan Mecham Eco Terrorist International Conspiracy  EMETIC   claimed responsibility for damaging the pylons and cables which supported the main ski chairlift at the Fairfield Snow Bowl Ski Resort in Flagstaff  Arizona in the United States    There were no casualties  but the property damage was estimated at  number number and an additional  number number to heighten security at the resort  In a letter to the ski resort  the group stated that  if trees were not allowed to grow back  and  ski operations not discontinued   then more damage would occur  Marc Leslie Davis  Margaret Katherine Millet  Ilse Washington Asplund  and Marc Andre Baker were arrested and in number  pled guilty and sentenced for their involvement in this incident '\n",
        "Additional Whitespaces\n",
        "As we applied Regular Expressions (like this one on session 6.1.3), we generated some additional whitespaces. We can threat it with RegEx as well.\n",
        "\n",
        "# Removing additional whitespaces\n",
        "corpus_after_regex = []\n",
        "for c in corpus_text:\n",
        "    c = re.sub(r'\\s+', ' ', c)\n",
        "    corpus_after_regex.append(c)\n",
        "    \n",
        "corpus_after_regex[1113]\n",
        "'Members of the Evan Mecham Eco Terrorist International Conspiracy EMETIC claimed responsibility for damaging the pylons and cables which supported the main ski chairlift at the Fairfield Snow Bowl Ski Resort in Flagstaff Arizona in the United States There were no casualties but the property damage was estimated at number number and an additional number number to heighten security at the resort In a letter to the ski resort the group stated that if trees were not allowed to grow back and ski operations not discontinued then more damage would occur Marc Leslie Davis Margaret Katherine Millet Ilse Washington Asplund and Marc Andre Baker were arrested and in number pled guilty and sentenced for their involvement in this incident '\n",
        "Good! I think we are done here with RegEx\n",
        "\n",
        "Lower Case\n",
        "The next step we must on Natural Language Processing is putting all the tokens in lower case. We can do that with the method apply of Pandas DataFrame.\n",
        "\n",
        "cleaned_corpus = pd.Series(corpus_after_regex).apply(lambda x: x.lower())\n",
        "cleaned_corpus = list(cleaned_corpus.values)\n",
        "cleaned_corpus[990]\n",
        "'the vietnamese organization to exterminate communists and restore the nation claimed responsibility for shooting nguyen van luy and his wife pham thi luu in front of their residence in san francisco california united states van luy a vocal defender of vietnam s communist government was wounded and thi luu was killed '\n",
        "WordCloud\n",
        "In this session, we will generate a WordCloud for all descriptions.\n",
        "\n",
        "\n",
        "I hope you really enjoy this storytelling. Please upvote this kernel to keep me motivated to do even more!\n",
        "\n",
        "This is not the final version. There is much more to do:\n",
        "\n",
        "Create a WordCloud for attributes like corp1, target1 and motive;\n",
        "Look for more Exploratory Data Analysis like:\n",
        "Incidents that lasted more than 24h (extended = 1);\n",
        "Major radical groups responsible for terrorist attacks (gname);\n",
        "Attacks with the highest number of terrorists (nperps);\n",
        "References\n",
        "https://nbviewer.jupyter.org/gist/jtbaker/57a37a14b90feeab7c67a687c398142c?flush_cache=true\n",
        "\n",
        "https://github.com/python-visualization/folium/issues/904\n",
        "\n",
        "https://towardsdatascience.com/data-101s-spatial-visualizations-and-analysis-in-python-with-folium-39730da2adf\n",
        "\n",
        "https://www.kaggle.com/rachan/how-to-folium-for-maps-heatmaps-time-analysis\n",
        "\n",
        "https://python-visualization.github.io/folium/plugins.html\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}